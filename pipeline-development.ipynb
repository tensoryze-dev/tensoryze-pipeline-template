{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Pipeline Development**\n",
    "\n",
    "This repository serves as a scaffold for the interactive and experimental-friendly development of Data and ML pipelines. The development of this environment was guided by the following philosophy:\n",
    "\n",
    "**Please read the README.md carefully before starting the development.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Create Folder Structure for Pipeline Components**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensoryze_pipelines import MLPipelineSetup\n",
    "\n",
    "MLPipelineSetup.setup_pipeline_structure(\n",
    "    pipeline_step_names=[\"ingestion\", \"train\", \"test\", \"publish\"]\n",
    ")\n",
    "MLPipelineSetup.load_env_vars()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Data preparation: app.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile components/ingestion/app.py\n",
    "import glob, os, dotenv\n",
    "from tensoryze_pipelines.io.interfaces import DataLakeInterface\n",
    "from tensoryze_pipelines.io.datalake.clients import LakeFSClient\n",
    "from tensoryze_pipelines.io.datalake.ml import MachineLearningDataLakeClient\n",
    "client = LakeFSClient(interface=DataLakeInterface())\n",
    "client = MachineLearningDataLakeClient(client)\n",
    "branches = client.create_train_test_branch(test_ratio=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **dockerfile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile components/ingestion/dockerfile\n",
    "\n",
    "# Define base image\n",
    "FROM python:3.10-slim as base\n",
    "\n",
    "# Set environment variables\n",
    "ENV VIRTUAL_ENV=/opt/venv\n",
    "ENV PATH=\"$VIRTUAL_ENV/bin:$PATH\"\n",
    "\n",
    "# Build stage for cloning the repository\n",
    "FROM base as builder\n",
    "\n",
    "RUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*\n",
    "RUN python3 -m venv $VIRTUAL_ENV\n",
    "\n",
    "WORKDIR /code\n",
    "\n",
    "ARG GITHUB_INSTALL_TOKEN\n",
    "\n",
    "COPY requirements.txt .\n",
    "\n",
    "# install dependencies\n",
    "RUN pip install git+https://${GITHUB_INSTALL_TOKEN}@github.com/tensoryze-dev/tensoryze_pipelines.git#egg=tensoryze_pipelines && \\\n",
    "    pip install -r requirements.txt && \\\n",
    "    rm -rf ~/.cache/pip\n",
    "\n",
    "\n",
    "COPY app.py .\n",
    "COPY .env .\n",
    "\n",
    "CMD [\"python\", \"./app.py\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Modell Training**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **inference_processing.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting inference_artifacts/inference_preprocessing.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile inference_artifacts/inference_preprocessing.py\n",
    "import torch\n",
    "from PIL import Image\n",
    "from tensoryze_pipelines.modeling import OpticalInspectionTransformation, TensorPILImageConverter\n",
    "from tensoryze_service.datamodel import TensoryzeEvent\n",
    "\n",
    "#to be used in inference service\n",
    "def inference_preprocessing(event: TensoryzeEvent) -> Image.Image:\n",
    "    data = event.get_data()\n",
    "    TR = OpticalInspectionTransformation(img_size = 224) #, crop = [set_y, set_x, w_size, w_size])\n",
    "    data: torch.Tensor = TR.transform(data).unsqueeze(0)\n",
    "    return TensorPILImageConverter.convert(data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "notebookRunGroups": {
     "groupValue": "2"
    }
   },
   "source": [
    "##### **app.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/train/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/train/app.py\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import  os, glob\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from tensoryze_pipelines.modeling import (\n",
    "    OpticalInspectionDataloader, OpticalInspectionDataset,  OpticalInspectionTransformation, \n",
    "    TemperatureScaledActiveLearner, ResNet18\n",
    ")\n",
    "from tensoryze_pipelines.utils.logging import logger\n",
    "from tensoryze_pipelines.io import (\n",
    "    DataLakeInterface, LakeFSClient, MachineLearningDataLakeClient,\n",
    "    write_dict_to_yaml, read_pickle, write_json, image_folder_to_dataset,\n",
    "    ExperimentTrackingInterface, MLFlowExperimentTracking, \n",
    ")\n",
    "\n",
    "from tensoryze_pipelines.entities.execution import ModelArtifact\n",
    "\n",
    "N_EPOCHS = os.environ.get('N_EPOCHS', 10)\n",
    "\n",
    "log = logger\n",
    "experiment_tracker = MLFlowExperimentTracking(ExperimentTrackingInterface())\n",
    "client = MachineLearningDataLakeClient(\n",
    "    LakeFSClient(interface=DataLakeInterface()) \n",
    ")\n",
    "local_path = client.download_dataset(folder = \"/tmp\")\n",
    "\n",
    "X_test, y_test = image_folder_to_dataset(local_path, subfolder=\"test-data\")  \n",
    "X_train, y_train = image_folder_to_dataset(local_path, subfolder=\"train-data\")  \n",
    "\n",
    "log.info(\"üõ†Ô∏è   preparing training data\")\n",
    "TR = OpticalInspectionTransformation(img_size = 224) #, crop = [set_y, set_x, w_size, w_size])\n",
    "DS_TRAIN = OpticalInspectionDataset(X_train, y_train, sensor = \"vision_line\", transform = TR.transform)\n",
    "DL_TRAIN = OpticalInspectionDataloader(DS_TRAIN, log=log, test_split=False)\n",
    "\n",
    "DS_TEST = OpticalInspectionDataset(X_test, y_test, sensor = \"vision_line\", transform = TR.transform)\n",
    "DL_TEST = OpticalInspectionDataloader(DS_TEST, log=log, test_split=False, val_split=False)\n",
    "\n",
    "DL_TEST.prepare_dataloaders()\n",
    "DL_TRAIN.prepare_dataloaders()\n",
    "\n",
    "log.info(\"‚úîÔ∏è   prepared training data\")\n",
    "\n",
    "# TODO: Log sample images to MLFlow\n",
    "img = DL_TRAIN.show_images(DL_TEST.testloader, n=10)\n",
    "\n",
    "with experiment_tracker as experiment_tracker:\n",
    "    experiment_tracker.log_figure(img, file_name=\"verify_data.png\")\n",
    "\n",
    "    early_stop_callback = EarlyStopping(monitor=\"val_loss\", min_delta=0.00, patience=50, verbose=False, mode=\"min\")\n",
    "    checkpoint_callback = ModelCheckpoint(dirpath='/tmp/best_after_fit', save_top_k=1, verbose=True, monitor='val_loss', mode='min') # Define a ModelCheckpoint callback to save the best model\n",
    "\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs=N_EPOCHS, \n",
    "        accelerator=\"gpu\", \n",
    "        devices=1,\n",
    "        callbacks=[\n",
    "            early_stop_callback,\n",
    "            checkpoint_callback\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    log.info(\"[ ] Starting training\")\n",
    "\n",
    "    trainer.fit(\n",
    "        model=ResNet18(), \n",
    "        train_dataloaders=DL_TRAIN.trainloader, \n",
    "        val_dataloaders=DL_TRAIN.valloader, \n",
    "    )\n",
    "\n",
    "    log.info(\"[x]  Training completed\")\n",
    "\n",
    "    best_model_checkpoint = checkpoint_callback.best_model_path # Load the best mode checkpoint into a variable\n",
    "    print(best_model_checkpoint)\n",
    "    loaded_model = ResNet18.load_from_checkpoint(best_model_checkpoint) # Load the best checkpointed model into a variable == uncalibrated model\n",
    "    loaded_model.eval()    \n",
    "    \n",
    "    active_learner = TemperatureScaledActiveLearner(loaded_model, DL_TRAIN)\n",
    "    al_infer_config = active_learner.get_config(manual_treshold=0.25)\n",
    "    write_json(al_infer_config, './inference_artifacts/al_infer_config.json')\n",
    "\n",
    "    log.info(\"[ ] logging inference artifacts...\")\n",
    "    for file in glob.glob(os.path.join(\"./inference_artifacts\", \"*.*\")):\n",
    "        experiment_tracker.log_artifact(file_name = file)\n",
    "        log.info(f\"[x] Logged to mlflow: {file}\")\n",
    "\n",
    "    log_names = [\"/tmp/model.pkl\",\"/tmp/transform.pkl\"]\n",
    "    objects = [trainer.model, TR]   \n",
    "    \n",
    "    for f_name, obj in zip(log_names, objects):\n",
    "        experiment_tracker.log_artifact(file_name = f_name, artifact=obj)\n",
    "\n",
    "\n",
    "    artifact = ModelArtifact(\n",
    "        artifact_name=ResNet18.__name__,\n",
    "        run_id = f\"{experiment_tracker.experiment_id}/{experiment_tracker.run_id}\"\n",
    "    )\n",
    "    artifact.write()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/train/dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/train/dockerfile\n",
    "FROM pytorch/pytorch:2.2.0-cuda12.1-cudnn8-runtime as base\n",
    "\n",
    "\n",
    "# Build stage for cloning the repository\n",
    "FROM base as builder\n",
    "\n",
    "WORKDIR /code\n",
    "\n",
    "RUN apt-get update && apt-get install -y libglib2.0-0 libgl1-mesa-glx && rm -rf /var/lib/apt/lists/*\n",
    "RUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "\n",
    "ARG GITHUB_INSTALL_TOKEN\n",
    "COPY requirements.txt .\n",
    "# install dependencies\n",
    "RUN pip install git+https://${GITHUB_INSTALL_TOKEN}@github.com/tensoryze-dev/tensoryze_pipelines.git#egg=tensoryze_pipelines[ml] && \\\n",
    "    pip install -r requirements.txt && \\\n",
    "    rm -rf ~/.cache/pip\n",
    "\n",
    "\n",
    "COPY inference_artifacts/ ./inference_artifacts\n",
    "COPY app.py .\n",
    "COPY .env .\n",
    "\n",
    "# command to run on container start\n",
    "CMD [ \"python\", \"./app.py\" ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Model Testing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **app.py**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/test/app.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/test/app.py\n",
    "import pickle, os\n",
    "from tensoryze_pipelines.entities.execution import ModelArtifact, DeploymentRule\n",
    "from tensoryze_pipelines.deployment import read_local_model_id, DeploymentSolver\n",
    "from tensoryze_pipelines.ml_testing import (\n",
    "    TestFactory, MetricFactory, RunManager, TransformationFactory\n",
    ")\n",
    "from tensoryze_pipelines.modeling import OpticalInspectionDataloader, OpticalInspectionDataset, OpticalInspectionTransformation\n",
    "from tensoryze_pipelines.io import (\n",
    "    image_folder_to_dataset, ExperimentTrackingInterface, LakeFSClient, DataLakeInterface, MachineLearningDataLakeClient,\n",
    ")\n",
    "from tensoryze_pipelines.utils.logging import logger\n",
    "\n",
    "\n",
    "model_artifact = ModelArtifact.read()\n",
    "NEW_MODEL_ID = model_artifact.run_id\n",
    "\n",
    "TEST_SPECIFICATION = r\"testing_artifacts/test_specification.yaml\"\n",
    "\n",
    "\n",
    "client = LakeFSClient(interface=DataLakeInterface()) \n",
    "client = MachineLearningDataLakeClient(client)\n",
    "\n",
    "_ = ExperimentTrackingInterface()\n",
    "\n",
    "parsed_model_id = NEW_MODEL_ID.split(\"/\")[1]\n",
    "\n",
    "with open(\"/tmp/model.pkl\", 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "local_path = client.download_dataset(folder = \"/tmp\", branch_name=client.test_branch_name)\n",
    "X_test, y_test = image_folder_to_dataset(local_path, subfolder=client.test_branch_name)\n",
    "  \n",
    "logger.info(\"[ ] preparing training data\")\n",
    "TR = OpticalInspectionTransformation(img_size = 224)\n",
    "DS_TEST = OpticalInspectionDataset(X_test, y_test, sensor = \"vision_line\", transform = TR.transform)\n",
    "DL_TEST = OpticalInspectionDataloader(DS_TEST, log=logger, test_split=False, val_split=False)\n",
    "\n",
    "logger.info(\"[x] prepared training data\")\n",
    "\n",
    "transformation_factory = TransformationFactory(224, None, DS_TEST, OpticalInspectionDataloader)\n",
    "metric_factory = MetricFactory()\n",
    "test_factory = TestFactory(model, transformation_factory, metric_factory)\n",
    "\n",
    "run_manager = RunManager(specification_path=TEST_SPECIFICATION, model=model, test_factory=test_factory)\n",
    "run_manager.execute_tests(run_id = parsed_model_id)\n",
    "\n",
    "solver = DeploymentSolver(\n",
    "    test_results=run_manager.test_results,\n",
    "    hierarchy=run_manager.hierarchy_list,\n",
    ")\n",
    "solver.solve()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **dockerfile**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting components/test/dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%writefile components/test/dockerfile\n",
    "\n",
    "FROM pytorch/pytorch:2.2.0-cuda12.1-cudnn8-runtime as base\n",
    "\n",
    "# Build stage for cloning the repository\n",
    "FROM base as builder\n",
    "\n",
    "WORKDIR /code\n",
    "\n",
    "RUN apt-get update && apt-get install -y libglib2.0-0 libgl1-mesa-glx && rm -rf /var/lib/apt/lists/*\n",
    "RUN apt-get update && apt-get install -y git && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "\n",
    "ARG GITHUB_INSTALL_TOKEN\n",
    "COPY requirements.txt .\n",
    "# install dependencies\n",
    "RUN pip install git+https://${GITHUB_INSTALL_TOKEN}@github.com/tensoryze-dev/tensoryze_pipelines.git#egg=tensoryze_pipelines[ml] && \\\n",
    "    pip install -r requirements.txt && \\\n",
    "    rm -rf ~/.cache/pip\n",
    "\n",
    "# copy the content of the local src directory to the working directory\n",
    "COPY app.py .\n",
    "COPY testing_artifacts/ ./testing_artifacts/\n",
    "COPY inference_artifacts/ ./inference_artifacts/\n",
    "COPY .env .\n",
    "\n",
    "\n",
    "# command to run on container start\n",
    "CMD [ \"python\", \"./app.py\" ] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **CT Pipeline Definition**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%writefile pipeline/manifest.json\n",
    "{\n",
    "    \"name\": \"pcb-demo-pipeline\",                                 \n",
    "    \"kind\": \"ml-job\",                                                        \n",
    "    \"pipeline_steps\": {\n",
    "        \"ingestion\": {\n",
    "            \"name\": \"ingest_pcb_data\", \"image\": \"pcb-demo-data/ingestion:latest\"\n",
    "        },\n",
    "        \"train\": {\n",
    "            \"name\": \"train_resnet\", \"image\": \"pcb-demo-data/train:latest\"\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"name\": \"test_robustness\", \"image\":\"pcb-demo-data/test:latest\"\n",
    "        }\n",
    "    },\n",
    "    \"pipeline_dag\": {\n",
    "        \"root\": [\"ingestion\"],\n",
    "        \"ingestion\": [\"train\"],\n",
    "        \"train\": [\"test\"],\n",
    "        \"test\": [\"end\"]\n",
    "    },\n",
    "    \"scheduler\": {\n",
    "        \"kind\": \"TimeRESTScheduler\",\n",
    "        \"condition\": \"* * *1 * *\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment of Pipeline\n",
    "1. make local_run\n",
    "2. make push_to_registry\n",
    "3. make register"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
